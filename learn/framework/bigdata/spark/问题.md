#   问题
*   RowMatrix  65535
    -   This cannot be computed on matrices with more than 65535 columns.
    -   只能处理65535维的数据
*   vituralbox 启动spark集群，当result过大时，driver需要通过blockmanager 获取，由于端口不是固定的所以无法配置端口映射
    -   17/11/28 09:49:24 ERROR RetryingBlockFetcher: Exception while beginning fetch of 1 outstanding blocks 
java.io.IOException: Failed to connect to /172.17.0.2:45442
    -   解决方案 https://stackoverflow.com/questions/44935477/spark-2-broadcast-inside-docker-uses-random-port
```
docker run -ti --rm \
    --name sparkmaster \
    --hostname=host.a \
    --add-host host.a:xx.xx.xx.xx \
    --add-host host.b:xx.xx.xx.xx \
    -p 18080:18080 \

    .config("spark.fileserver.port", "7002") \
    .config("spark.broadcast.port", "7003") \
    .config("spark.replClassServer.port", "7004") \
    .config("spark.blockManager.port", "7005") \
    .config("spark.broadcast.blockSize", "4096") \
    .getOrCreate()

    //spark.driver.maxResultSize
    // spark.task.maxDirectResultSize

     conf.getSizeAsBytes("spark.task.maxDirectResultSize", 1L << 20),
    RpcUtils.maxMessageSizeBytes(conf))
     private val MAX_MESSAGE_SIZE_IN_MB = Int.MaxValue / 1024 / 1024
      ==>
          .config("spark.task.maxDirectResultSize", "2147483648") \
    .config("spark.driver.maxResultSize","3g") \

```
*   python pandas datafram语法问题：
    -   group_by_post_colums_sum = data_T.apply(lambda x:sum(x))
```
  File "/Users/seki/git/dyd/data_foundation/python/com/dyd/data/test/TestRecommend.py", line 158, in <lambda>
    group_by_post_colums_sum = data_T.apply(lambda x:sum(x))
  File "/Users/seki/git/learn/spark/python/lib/pyspark.zip/pyspark/sql/functions.py", line 40, in _
    jc = getattr(sc._jvm.functions, name)(col._jc if isinstance(col, Column) else col)
AttributeError: ("'NoneType' object has no attribute '_jvm'", u'occurred at index 6293615542250297450')
```


##    spark on yarn  spark ui
-   7/11/29 11:25:34 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> uhadoop-adarbt-master1,uhadoop-adarbt-master2, PROXY_URI_BASES -> http://uhadoop-adarbt-master1:23188/proxy/application_1511859350454_0038,http://uhadoop-adarbt-master2:23188/proxy/application_1511859350454_0038), /proxy/application_1511859350454_0038
17/11/29 11:25:34 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
*   通过这个访问
    -   https://hadoop.diyidan.net:23188/proxy/application_1511859350454_0038


##   17/05/03 17:58:02 ERROR client.TransportClient: Failed to send RPC 7785784597803174149 to /172.26.159.91:56630: java.nio.channels.ClosedChannelException 
*   http://duguyiren3476.iteye.com/blog/2372479
*   单个executor 使用的 cpu 不能超过5个 （dyd ucloud集群的情况）


## spark on yarn cdh 5.49 Found both spark.driver.extraClassPath and SPARK_CLASSPATH. Use only the former.
*  startshell  --driver-class-path  $CONF_DIR:$LIB_JARS == 》export SPARK_CLASSPATH="$SPARK_CLASSPATH:$CONF_DIR:$LIB_JARS"

## spark broadcast  Failed to get broadcast_8_piece0 of broadcast_8
*  spark 初始化 不能是成员变量


## breeze Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
*  https://github.com/fommil/netlib-java
*  https://stackoverflow.com/questions/37848216/how-to-configure-high-performance-blas-lapack-for-breeze-on-amazon-emr-ec2
*  update-alternatives: using /usr/lib/atlas-base/atlas/libblas.so.3 to provide /usr/lib/libblas.so.3 (libblas.so.3) in auto mode
Setting up libopenblas-base (0.2.8-6ubuntu1) ...
update-alternatives: using /usr/lib/openblas-base/libblas.so.3 to provide /usr/lib/libblas.so.3 (libblas.so.3) in auto mode
* sudo ln -s /usr/lib/openblas-base/liblapack.so.3 /usr/lib/liblapack.so.3
