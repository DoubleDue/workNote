import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.hadoop.hbase.spark.HBaseContext
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{Cell, CellUtil, HBaseConfiguration, TableName}
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import java.util
import com.dyd.data.offline.daily.post.BaseDataDf
import com.dyd.data.offline.daily.post.bean.DailyPostStatResult
import com.dyd.data.offline.daily.post.constant.CommonConstants
import org.apache.hadoop.hbase.client.{Get, Result}
import org.apache.hadoop.hbase.spark.HBaseContext
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{Cell, CellUtil, HBaseConfiguration, TableName}
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import org.slf4j.LoggerFactory
import java.security.MessageDigest
import java.util
import java.util.Properties
import scala.collection.mutable.ArrayBuffer

val targetDay ="2018-01-26"
def loadHiveTableData(spark:SparkSession,table:String,fileds:Array[String]):DataFrame = {
    val df: DataFrame = spark.sql(s"select ${fileds.mkString(",")} from dyd_raw_data.${table} where day = '${targetDay}' ")
    df.cache()
    df
  }

case class Profile(attribute: String, tag: String)

 val targetActionDf = loadHiveTableData(spark,"read_action",Array("userId","postId","duration","completion"))


  var rdd = targetActionDf.select("postId").distinct().cache().rdd.mapPartitions(iter => {
      val digest = MessageDigest.getInstance("MD5")
      val result = new ArrayBuffer[Array[Byte]]()
      iter.foreach(row => {
        val id = row.getAs[Long]("postId")
        val rowKey = digest.digest(s"$id".getBytes).map("%02x".format(_)).mkString
        result += Bytes.toBytes(rowKey)
        digest.reset()
      })
      result.iterator
    })


    val postProfileCategory = "CG"
    val postProfileMediaType = "MT"
    val postProfileCreateTime = "CT"

  import java.text.SimpleDateFormat
    val COMPARE_TIME = new SimpleDateFormat(
    "yyyy-MM-dd HH:mm:ss").parse(s"$targetDay 23:59:59").getTime()

  val TIME_RANGE = Array((0, 0), (1, 3), (4, 7), (8, 15), (16, 30), (31, 60), (61, 90), (91, 180), (181, 270), (271, 360), (361, 720), (721, 10000))

   def getRegTimeRange(time: Long): String = {
    val days = ((COMPARE_TIME - time) / (1000 * 3600 * 24)).toInt
    val filter = TIME_RANGE.filter(r => {
      r._1 <= days && days <= r._2
    })(0)
    s"${filter._1}-${filter._2}"
  }


  val conf = HBaseConfiguration.create()
    val hbaseContext = new HBaseContext(sc, conf)

    val profileDf = hbaseContext.bulkGet[Array[Byte], Array[Profile]](
      TableName.valueOf("dyd:post_dim"),
      1000,
      rdd,
      record => {
        new Get(record)
      },
      (result: Result) => {
        val profileArr = new ArrayBuffer[Profile]()
        if(result != null) {
          val listCells: util.List[Cell] = result.listCells()
          if(listCells != null ) {
            val it = listCells.iterator()
            while (it.hasNext) {
              val cell = it.next()
              val column = Bytes.toString(CellUtil.cloneQualifier(cell))
              if ("CG".equals(column) || "CT".equals(column)) {
                profileArr += Profile(column, Bytes.toInt(CellUtil.cloneValue(cell)).toString())
              }
  val TIME_RANGE = Array((0, 0), (1, 3), (4, 7), (8, 15), (16, 30), (31, 60), (61, 90), (91, 180), (181, 270), (271, 360), (361, 720), (721, 10000))
              if ("CT".equals(column)) {
                val regTime: Long = Bytes.toLong(CellUtil.cloneValue(cell))
                val targetDay ="2018-01-26"

                 val COMPARE_TIME = new SimpleDateFormat(
    "yyyy-MM-dd HH:mm:ss").parse(s"$targetDay 23:59:59").getTime()
                val days = ((COMPARE_TIME - regTime) / (1000 * 3600 * 24)).toInt
    val filter = TIME_RANGE.filter(r => {
      r._1 <= days && days <= r._2
    })
    if(filter.length > 0){
       profileArr += Profile(column, s"${filter(0)._1}-${filter(0)._2}")
    } else {
        profileArr += Profile("NULL",regTime.toString())
    }
              }
            }
          }
        }
        profileArr.toArray
      }).filter(_.length > 0).mapPartitions(iter => {
      val rs = new ArrayBuffer[Profile]()
      iter.foreach(row => {
        row.foreach(p => {
          rs += p
        })
      })
      rs.toIterator
    }).toDF().cache()


  val distributionDf: DataFrame = spark.sql(s"select userId,postId,reason  from dyd_raw_data.post_distribution where day = '${targetDay}' ")
    distributionDf.cache()





reasonCountMap.foreach(f =>{
	val targetReasonUsersDf  = distributionDf.filter(s" reason ='${f._1}' ").select(distributionDf("userId").as("d_userId")).distinct().cache()
	println(targetReasonUsersDf.count())
})

  18/01/03 15:42:07 DEBUG ProtobufRpcEngine: Call: getListing took 0ms
18/01/03 15:42:07 DEBUG AcidUtils: in directory hdfs://Ucluster/user/hive/warehouse/dyd_raw_data.db/read_action/day=2018-01-03 base = null deltas = 6

hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager
hive.compactor.initiator.on = true
hive.compactor.worker.threads = 5


      SparkSession.builder.master("yarn").enableHiveSupport().appName("test2").getOrCreate()

      CREATE TABLE post_action(userId BIGINT,postId BIGINT,actionType INT ,position INT ,module TINYINT,source TINYINT ,createTime TIMESTAMP)
 PARTITIONED BY(day STRING)
 CLUSTERED BY(userId)  INTO 32 BUCKETS
 ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
 STORED AS ORC tblproperties("transactional"="true","compactorthreshold.hive.compactor.delta.num.threshold"="5","compactor.mapreduce.map.memory.mb"="2048","compactorthreshold.hive.compactor.delta.pct.threshold"="0.01","orc.compress"="SNAPPY");

 alter table post_action PARTITION(day='2018-01-04') COMPACT 'major';
  alter table post_action PARTITION(day='2018-01-04') COMPACT 'major';
  alter table post_action PARTITION(day='2018-01-04') COMPACT  'minor';

 alter table dyd_raw_data.post_distribution PARTITION(day='2018-02-10') COMPACT 'major';
  alter table dyd_raw_data.read_action PARTITION(day='2018-01-23') COMPACT 'major';

SHOW COMPACTIONS;

day=2018-01-04 hive -e "SELECT * FROM default.post_action WHERE day = ${env:day}";

 hive -e "alter table post_action PARTITION(day='2018-01-04') COMPACT 'major'"

 hive -e " drop table dyd_raw_data.post_exposure "
 hive -e " drop table dyd_raw_data.post_exposure "
 hive -e " drop table dyd_raw_data.post_exposure "
 hive -e " drop table dyd_raw_data.post_exposure "

0 */2 * * * ./bash_profile;/bin/sh /root/dyd_data_foundation/shells/flume/hive_compact_task.sh dyd_raw_data.post_distribution
*/10 * * * * ./bash_profile;/bin/sh /root/dyd_data_foundation/shells/flume/hive_compact_task.sh dyd_raw_data.read_action 

 alter table dyd_raw_data.post_action PARTITION(day='2018-01-17') COMPACT 'major';

 alter table dyd_raw_data.post_exposure PARTITION(day='2018-01-17') COMPACT 'major';
